from fastapi import APIRouter, File, UploadFile, Form, HTTPException
from app.schemas.chat_schema import ChatResponse
from app.services.rag_pipeline import rag_process
from loguru import logger

router = APIRouter()

@router.post("/chat_llm", response_model=ChatResponse)
async def chat_llm(file: UploadFile = File(...), query: str = Form(...), prompt: str = Form(...)):
    try:
        logger.info(f"Received chat request: file={file.filename}, query={query}, prompt={prompt}")
        
        chunks = await rag_process(file, query)
        logger.debug(f"Chunks received for chat: {chunks}")  # üëà –õ–æ–≥–∏—Ä—É–µ–º, —á—Ç–æ –≤–µ—Ä–Ω—É–ª–æ—Å—å

        # –ü—Ä–æ–≤–µ—Ä–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É chunks –ø–µ—Ä–µ–¥ —Å–±–æ—Ä–∫–æ–π context
        if not chunks or not isinstance(chunks, list) or "chunk" not in chunks[0]:
            logger.warning("Invalid chunks structure")
            raise ValueError("Invalid chunks structure")

        context = "\n".join(chunk["chunk"] for chunk in chunks)

        # –ú–æ–∫-–æ—Ç–≤–µ—Ç (–ø–æ–∫–∞ –±–µ–∑ –≤—ã–∑–æ–≤–∞ OpenAI API)
        mocked_response = f"<Mocked response on query: '{query}' with prompt: '{prompt}' and {len(chunks)} chunks>"
        return {"response": mocked_response}

    except Exception as e:
        logger.exception("Chat error")  # üëà –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π traceback
        raise HTTPException(status_code=500, detail="Internal server error")

